{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeadDkMiISin"
      },
      "source": [
        "# Gemini API: General Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook covers:\n",
        "\n",
        "- Temperature\n",
        "- Max output length\n",
        "- Token counting\n"
      ],
      "metadata": {
        "id": "QOKxmDLtRogx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFTEz0qtFvxC"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSAK8IsGF4Os"
      },
      "source": [
        "Since we've put our gemini API key in Colab Secrets, we can just run the following cells to setup:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OEoeosRTv-5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a642cc08-7a3c-4783-b08e-3ea5abe16b5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.9/163.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m718.3/718.3 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q -U google-generativeai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEXQ3OwKIa-O"
      },
      "source": [
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Safety.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TS9l5igubpHO"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ab9ASynfcIZn"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZfoK3I3hu6V"
      },
      "source": [
        "## Model Temperature\n",
        "\n",
        "Steve is indecisive about how to spend his Friday night as a Freshman at Berkeley. He decides to ask Gemini using the 1.5 Flash [variant](https://ai.google.dev/gemini-api/docs/models/gemini):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "2bcfnGEviwTI",
        "outputId": "e477acb2-282a-446d-b28a-383a30afa8c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Challenge your ABU Computer Engineering friends to an epic online gaming showdown, putting those sharp analytical skills to dominate virtual realms. Alternatively, dive into a fun personal tech project you've always dreamed of, purely for the joy of creating and problem-solving.\n"
          ]
        }
      ],
      "source": [
        "model_name = 'gemini-2.5-flash'\n",
        "model = genai.GenerativeModel(model_name)\n",
        "\n",
        "prompt = \"Help me choose a fun way to spend my Friday night as a graduate of computer engineering at ABU ,Zaria in 2 lines\"\n",
        "response = model.generate_content(prompt)\n",
        "print(response.text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJwOhbdVkD_c"
      },
      "source": [
        "Steve is very smart and knows that Gemini is non-deterministic; the same prompt can result in different outputs! To demonstrate this, the following code passes the same prompt 5 different times:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "oxc8f6oQlCAk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "3afd51fb-d9ed-4900-81b6-fe7f47b20a46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Host an epic multiplayer video game tournament with your engineering friends, complete with victory snacks and bragging rights.\n",
            "2. Spend your Friday night organizing a fun, low-stakes coding challenge or a competitive strategy board game session with fellow ABU CE alumni, complete with local Zaria treats and plenty of friendly banter.\n",
            "3. Organize a \"useless machine\" mini-hackathon with fellow ABU CE grads, collaboratively building a hilariously absurd IoT gadget powered by pizza, witty banter, and friendly debugging battles.\n",
            "4. Spend your Friday night automating something hilariously useless or diving into a quirky, low-pressure coding project purely for amusement and the joy of tinkering.\n",
            "5. Organize a low-stakes, high-fun competitive gaming night with your fellow ABU Computer Engineering grads, fueled by snacks and witty tech banter.\n"
          ]
        }
      ],
      "source": [
        "outputs = []\n",
        "prompt = \"Help me choose a fun way to spend my Friday night as a graduate of computer engineering at ABU ,Zaria in in a single one-sentence idea\"\n",
        "for i in range(5):\n",
        "  response = model.generate_content(prompt)\n",
        "  outputs.append(response.text)\n",
        "for index, sentence in enumerate(outputs, start=1):\n",
        "    print(f\"{index}. {sentence}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fA9ewE_dMFMZ"
      },
      "source": [
        "Gemini returns a different response each time. Hmm. Steve doesn't like the fact that his Friday night might be determined by random chance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONFTcYStQ2YY"
      },
      "source": [
        "Fortunately, Gemini has a temperature parameter that controls the randomness of the output. Temperature values can range from 0.0 to 2.0. We can check the temperature of our current model as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "GPndrGi2nP5j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "fb46799a-4bf9-4130-b135-cbaaff6671f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model(name='models/gemini-2.5-flash',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 2.5 Flash',\n",
            "      description=('Stable version of Gemini 2.5 Flash, our mid-size multimodal model that '\n",
            "                   'supports up to 1 million tokens, released in June of 2025.'),\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n"
          ]
        }
      ],
      "source": [
        "for m in genai.list_models():\n",
        "    if m.name == 'models/gemini-2.5-flash':\n",
        "        print(m)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBdqPso3kamW"
      },
      "source": [
        "Let's initialize a new model with a temperature of 0:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "-GqupV-dYsvg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "bb398911-e369-4c85-9057-61f71adf3bd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Gather your ABU CE squad for a virtual gaming showdown or a collaborative coding challenge, fueled by local snacks, good banter, and a killer Afrobeats playlist.\n",
            "2. Gather your fellow ABU techies for a competitive board game night, where strategic thinking and friendly rivalry fuel the fun over pizza and code talk.\n",
            "3. Gather your ABU CE squad for a virtual gaming showdown or a collaborative coding challenge, fueled by local snacks, good banter, and a killer Afrobeats playlist.\n",
            "4. Gather your fellow ABU techies for a competitive board game night, where strategic thinking and friendly rivalry fuel the fun over pizza and code talk.\n",
            "5. Gather your ABU CE squad for a virtual gaming showdown or a collaborative coding challenge, fueled by local snacks, good banter, and a killer Afrobeats playlist.\n"
          ]
        }
      ],
      "source": [
        "model_name = 'gemini-2.5-flash'\n",
        "new_outputs = []\n",
        "low_temp_model = genai.GenerativeModel(model_name, generation_config={\"temperature\": 0})\n",
        "for i in range(5):\n",
        "  response = low_temp_model.generate_content(prompt)\n",
        "  new_outputs.append(response.text)\n",
        "for index, sentence in enumerate(new_outputs, start=1):\n",
        "    print(f\"{index}. {sentence}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "All the outputs are the same! Finally Steve can be sure how to spend his night. Conversely, setting temperature to the max value of 2.0 would have the opposite effect, yielding more unpredictable responses."
      ],
      "metadata": {
        "id": "AZ78-JuN7XHi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5QDIG6cPbID"
      },
      "source": [
        "## Max Output Length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8MfrBmbPi5i"
      },
      "source": [
        "Let's say Steve wants his outputs to be below a certain length. In large language models, text is generated in tokens. For Gemini models, a token is equivalent to about 4 characters. 100 tokens are about 60-80 English words. He can set the `max_output_tokens` variable as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "he-OfzBbhACQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2e78e691-9fc8-4b03-b951-7e65e14ad9f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grab some friends and head\n"
          ]
        }
      ],
      "source": [
        "model_name = 'gemini-1.5-flash'\n",
        "short_response_model = genai.GenerativeModel(model_name, generation_config={\"max_output_tokens\": 5})\n",
        "prompt = \"Help me choose a fun way to spend my Friday night as a CS major at Berkeley in a single one-sentence idea\"\n",
        "response = short_response_model.generate_content(prompt)\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tz31PRYSRMUt"
      },
      "source": [
        "Notice that this simply halts token generation at a fixed quantity and does not guarantee that the output is complete."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4672af98ac57"
      },
      "source": [
        "## Token Count\n",
        "\n",
        "Let's say that Steve is being charged for every token that he inputs to Gemini.\n",
        "\n",
        "If Steve has billing enabled, the price of a paid request is controlled by the number of input and output tokens, so knowing how to count your tokens is important. As such, Steve might want to know the number of tokens in his prompt (input) before actually putting it into the model.\n",
        "\n",
        "Let's create a new instance of Gemini 1.5 Flash and set our prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "338fb9a6af78"
      },
      "outputs": [],
      "source": [
        "model_name = 'gemini-1.5-flash'\n",
        "token_n_model = genai.GenerativeModel(model_name, generation_config={\"temperature\": 0.0})\n",
        "poem_prompt = \"Write me a poem about Berkeley's campus\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = token_n_model.generate_content(poem_prompt)\n",
        "response.text"
      ],
      "metadata": {
        "id": "9WVlpLcDLjOs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "92e15a7b-b020-4c47-9a51-3bc26bcd14db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Where golden hills meet azure sky,\\nA campus sprawls, a vibrant sigh.\\nBerkeley, a name that echoes bold,\\nOf minds ablaze, stories untold.\\n\\nThe Campanile, a tower tall,\\nIts chimes resound, answering the call\\nOf students thronging, eager to learn,\\nIn classrooms bright, where knowledge burns.\\n\\nSather Gate, a grand archway,\\nLeads to paths where dreams hold sway.\\nFrom Sproul Plaza, a bustling scene,\\nTo Bancroft's charm, a tranquil green.\\n\\nThe scent of eucalyptus fills the air,\\nAs squirrels scamper, without a care.\\nThe Golden Bear, a mascot proud,\\nRoams the grounds, a spirit endowed.\\n\\nFrom Wheeler Hall, a history deep,\\nTo Doe Library, where secrets sleep.\\nThe Greek Theatre, a stage of grace,\\nWhere laughter rings, and time finds space.\\n\\nA tapestry of cultures, rich and bright,\\nA melting pot, day and night.\\nBerkeley, a haven for the free,\\nWhere minds ignite, and spirits flee.\\n\\nSo let us wander, explore, and dream,\\nIn this hallowed space, a vibrant stream.\\nFor Berkeley's heart, a beating drum,\\nA symphony of knowledge, yet to come. \\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before generating a response, we can check how many tokens are in this prompt using the `.count_tokens` function of the model:"
      ],
      "metadata": {
        "id": "-1w3HL5nIyR6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_token_count = token_n_model.count_tokens(poem_prompt)\n",
        "output_token_count = token_n_model.count_tokens(response.text)\n",
        "print(f'Tokens in prompt: {prompt_token_count} \\n Estimated tokens in output {output_token_count}')"
      ],
      "metadata": {
        "id": "lJ0V8H2dIf9Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "1c951198-e71c-40a8-adb2-232dfb9e0e56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens in prompt: total_tokens: 9\n",
            " \n",
            " Estimated tokens in output total_tokens: 269\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "google": {
      "image_path": "/static/site-assets/images/docs/logo-python.svg",
      "keywords": [
        "examples",
        "gemini",
        "beginner",
        "googleai",
        "quickstart",
        "python",
        "text",
        "chat",
        "vision",
        "embed"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}